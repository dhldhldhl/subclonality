---
title: "Weekly_Process_150622"
author: "Dohun Lee"
date: '2022-06-17'
output: html_document
editor_options: 
  chunk_output_type: console
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pracma); library(ggplot2); library(ggpubr); library(reshape2); 
library(mixtools); library(fitdistrplus); library(dplyr); library(QDNAseq); 
require(gtools); library(gridExtra); library(devtools)
source_url("https://raw.githubusercontent.com/elakatos/liquidCNA/main/mixture_estimation_functions.R")
knitr::opts_knit$set(root.dir = '/Users/dhl/laheen Dropbox/DHL DHL/Cambridge/Internship/subclonality/')
setwd("/Users/dhl/laheen Dropbox/DHL DHL/Cambridge/Internship/subclonality/")
source("SCRIPTS/3_liquidCNA.R")
load("../DATA/bam_by_patient.RData")
```

This is a RMDmarkdown to track and show progress between 15/06/22 ~ 22/06/22.

There were 4 main things to tackle:

1. liquidCNA not working for patients with two time samples. 
    + TODO: try dummy column
2. liquidCNA for patients with 6+ time sample. Is there an alternative to calling subclonal segments (i.e., monotony)? 
    + TODO: try splitting them. research monotony
4. Relative subclonal ratio:
    + TODO: tmp, divide by total number of sample rather than topSample
3. Understand inference of absolute subclonal ratio. estimateRGaussfit. How is absolute ratio calculated from relative ratio?
    + TODO: go function line by line. Histogram plot of seg.top.

# 1. Two time sample

There are 22 patients with two time samples:

```{r twoTimeSample}
sample_num_patient <- sapply(1:80, function(x) length(bam_by_patient[[x]]))
two_samp_patients <- which(sample_num_patient == 2)

patient_ids[two_samp_patients] #patient ids of patients with two time samples
table(sample_num_patient)
```

**Currently, an error occurs at sample ordering and segmentation classification step., filterSegmentRatios().**

### alter filterSegmentRatios()
```{r alter filterSegmentRatios}
  # Filter relative segment values (compared to baseline) to discard clonal (nonchanging) segments that stay relatively constant
  # Input:
  # - segs = normalised, purity-corrected segment-wise CN matrix, segments as rows, samples as columns
  # - cutOff = threshold value to determine whether a segment is varying enough to be not-clonal
  # - method = minmax or sd, evaluate segments based on their sd or minimum/maximum values in samples
  # Returns: a filtered set of a segments that have values exceeding the threshold
  
  filterSegmentRatios <- function(segs, cutOff, method, base=1){
    segsExtended <- cbind(segs, base) # add a further column, corresponding to the baseline
    if (!(method %in% c('minmax','sd'))){
      # if method is none of the supported ones, return the original
      print('Filtering criterion not recognised')
      return(segs)
    }
    if (method=='minmax'){
      # compute the minimal and maximal ratio in each segment, and return the ones that are far enough
      thresh.min <- 1-cutOff
      thresh.max <- 1+cutOff
      rat.max <- apply(segsExtended/base,1,max)
      rat.min <- apply(segsExtended/base,1,min)
      return(segs[(rat.max>thresh.max | rat.min<thresh.min),])
    }
    if (method=='sd'){
      # compute the standard deviation of each segment, and return the ones that are varied enough
      rat.sd <- apply(segsExtended,1,sd)
      if(nCol == 1){
        return(data.frame(Sample2 = segs[(rat.sd > cutOff),drop=F]))
      } else {
        return(segs[(rat.sd > cutOff),,drop=F])
      }
    }
  }
```

```{r eg_twoTimeSample, echo = FALSE}
p_num <- 6

seg.df <- read.delim(paste0("../DATA/2_QDNA_CNout/segment_", patient_ids[p_num], ".txt"))
cn.df <- read.delim(paste0("../DATA/2_QDNA_CNout/raw_cn_", patient_ids[p_num], ".txt"))

seg.df <- seg.df[,-(1:4)]
cn.df <- cn.df[,-(1:4)]

colnames(seg.df) <- paste0("Sample", 1:ncol(seg.df))
colnames(cn.df) <- paste0("Sample", 1:ncol(cn.df))

#re-normalise raw and segmented CN values to be centred at CN=2
reNorm <- centreSegs(seg.df)
seg.df <- as.data.frame(t(t(seg.df)/reNorm)*2)
cn.df <- as.data.frame(t(t(cn.df)/reNorm)*2)

#generate a dataframe of ensemble segments
#contiguous sections of bins that are constant in ALL samples
segchange <- sapply(1:(nrow(seg.df)-1),
                    function(i) sum(seg.df[i,]!=seg.df[(i+1),])>0)
seg.data <- data.frame(start=c(1,which(segchange)+1),
                       end=c(which(segchange),length(segchange)+1))
seg.data$length <- seg.data$end - seg.data$start+1

#Filter out small segments
seg.sub <- subset(seg.data, length>12) #12 500kb bins
cat("Patient",patient_ids[p_num], '| Total number of segments retained: ',nrow(seg.sub))

#Update segment values by fitting a normal distribution to bins in the segment
seg.fit <- getNormalFitSegments(seg.sub, cn.df)

seg.cns <- seg.fit[[1]]
names(seg.cns) <- names(seg.df)

seg.df.upd <- data.frame(matrix(NA, ncol=ncol(seg.df), nrow=nrow(seg.df)))
names(seg.df.upd) <- names(seg.df); row.names(seg.df.upd) <- row.names(seg.df)

for(i in 1:nrow(seg.sub)){
  seg.df.upd[seg.sub$start[i]:seg.sub$end[i],] <- seg.cns[i,]
}

w = c(0.8,1,1,0.15,0.05) #weights of different CN states
maxCN=8 #assumed maximum CN 
adjVec = c(0.5,0.6,0.8,0.9,1,1.2,1.3,1.5,1.8,2) #smoothing kernel adjustments
pVec = seq(0.05, 0.5, by=0.005) #range of purity values to be evaluated

#Estimated optimal purity values stored in pHat.df
#purity vals that minimise mean and median
pHat.df <- data.frame(matrix(vector(),ncol=ncol(seg.df.upd), nrow=2))
names(pHat.df) <- names(seg.df.upd)
row.names(pHat.df) <- c('mean','median')

#The plots show the error of the fit over the range of purity values 
for(i in 1:ncol(seg.df.upd)){
  #for each time sample...
  
  x <- na.omit(seg.df.upd[,i]) #get rid of NA rows (i.e., segments with NAs)
  
  pFits <- as.data.frame(sapply(adjVec,
                                function(a) sapply(pVec,
                                                   function(p) evalPurityDensity(p,w,a,x,maxCN))))
  #each row: the 1 purity value evaluated
  #each column: 1 adjVec, a smoothing kernel adjustment
  #each value: error for given purity/adjVec
  
  pFits$p <- pVec
  mins <- c(pVec[which.min(apply(pFits[,1:length(adjVec)],1,mean))],
            pVec[which.min(apply(pFits[,1:length(adjVec)],1,median))])
  #which row (purity) has minimum mean error and minimum median error?
  
  pHat.df[,names(seg.df.upd)[i]] <- mins
}

#Correct CN values by purity
#removing CN contamination by normal
#eq being used: CN_corr = 1/p * (CN-2) + 2
pVec <- as.numeric(pHat.df[1,,drop=T])
seg.df.corr <- as.data.frame(t(t(seg.df.upd-2)*1/pVec)+2)
seg.cns.corr <- as.data.frame(t(t(seg.cns-2)*1/pVec)+2)

#Designate reference sample
#& calculate dCN
baseSample <- names(seg.df.corr)[1]
seg.dcn <- seg.cns.corr - seg.cns.corr[,baseSample]
seg.dcn.nonBase <- seg.dcn %>% select(-one_of(baseSample))

#Choose samples to investigate (all non-base samples by default) 
colToUse <- names(seg.dcn.nonBase)
#generate all possible permutations
nCol <- length(colToUse)
seg.dcn.toOrder <- seg.dcn.nonBase[,colToUse]
ordVec <- permutations(nCol,nCol,colToUse)

#Initial filtering to find best sample order 
#Set parameters to be used when evaluating sample order and segment monotony
filterMethod <- 'sd' #filter by saying... if sd < theta, segment = clonal
cutOffVec <- seq(0.025,0.5,by=0.005) #thetas to try for initial-filtering
epsilon <- 0.05 #error margin for monotony

#Diagnostic plot
#at different thetas shows:
#the proportion of subclonal segments (of non-clonal segments)
#total number of non-clonal (light blue)
#and subclonal (dark blue) segments
fitInfo <- list()
for (cutOff in cutOffVec){
  seg.dcn.Eval <- filterSegmentRatios(seg.dcn.toOrder, cutOff, filterMethod, 1)
  #Filter relative segment values (compared to baseline) to discard clonal 
  #(nonchanging) segments that stay relatively constant
  #Returns: a filtered set of a segments that have values exceeding the threshold
  
  best <- findBestOrder(seg.dcn.Eval, ordVec, epsilon, nCol, 0)
  #Input: segments where clonal segments are initially filtered
  #Returns: 
  #$cons: row-names of all segments considered, 
  #max: maximum fit gained, 
  #$ord: order with the maximum fit, 
  #$segs:segments monotone according to max fit
  fitInfo[[as.character(cutOff)]] <- best
}

```

```{r segmentClassification}
fitInfo.df <- data.frame(cutOff = cutOffVec,
                         maxFit = sapply(fitInfo, function(x) x$max),
                         segsAboveCut = sapply(fitInfo, function(x) length(x$cons)),
                         segsInOrder = sapply(fitInfo, function(x) max(sapply(x$segs, length))))
p1 <- ggplot(fitInfo.df, aes(x=cutOff, y=maxFit)) + geom_line(size=2,colour='darkgreen') + theme_bw() +
  labs(x='Clonal cut-off',y='Subclonal proportion (of non-clonal segments)') + 
  ggtitle(paste0('Patient ', patient_ids[p_num]))
p2 <- ggplot(fitInfo.df, aes(x=cutOff, y=segsAboveCut)) + geom_line(size=2,colour='deepskyblue3') +
  geom_line(aes(y=segsInOrder), size=2,colour='dodgerblue4') + theme_bw() +
  labs(x='Clonal cut-off',y='Total number of non-clonal and subclonal segments') + 
  ggtitle(paste0('Patient ', patient_ids[p_num]))
grid.arrange(p1,p2,nrow=1)
```

For example, patient 957 has two time samples. Looking at the segment classification with the diagnostic plot, subclonal proportion is 1.0 for all cutOFF values. This would be the result of not being able to test for monotony with just one sample. As a result, recommended cutOff is always the lowest value tested (i.e. 0.025) - as it retains the most number of segments. 

# 2. 6+ time sample

There are 7 patients with more than 6 time samples. They have the following number of time samples, respectively.

```{r tooManyTimeSample}
timely_patients <- which(sample_num_patient > 6)
timely_bam_num <- sample_num_patient[timely_patients]

patient_ids[timely_patients] #patient ids of patients with 6+ time samples
timely_bam_num #number of time samples they have
```

### Problem 1:

Patient 2789 has one erroneous time sample. As seen sample 7 has weird CN value. So this time sample was removed.

```{r patient2789}
seg.df <- read.delim(paste0("../DATA/2_QDNA_CNout/segment_2789_og", ".txt"))
cn.df <- read.delim(paste0("../DATA/2_QDNA_CNout/raw_cn_2789_og", ".txt"))

seg.df <- seg.df[,-(1:4)]
cn.df <- cn.df[,-(1:4)]
  
colnames(seg.df) <- paste0("Sample", 1:ncol(seg.df))
colnames(cn.df) <- paste0("Sample", 1:ncol(cn.df))

head(seg.df)
head(cn.df)

timely_bam_num[4] <- timely_bam_num[4] - 1 #remove sample7
```

### Coming back to the analysis...

Divide time samples into 2 batches with one overlap
```{r divideTimeSamples}
#divide time samples into 2 batches with one overlap
nbatch1 <- ceiling(timely_bam_num/2)
nbatch2 <- timely_bam_num - nbatch1
```

Run liquidCNA on both batches
```{r runliquidCNA in batch, eval = FALSE, echo = TRUE}
#initialise list to store results
timely_batch1_results <- vector(mode = "list", length = length(timely_patients))
timely_batch2_results <- vector(mode = "list", length = length(timely_patients))

#run liquidCNA on each batch
timely_batch1_results <- sapply(1:7, 
                                simplify = FALSE, 
                                function(x) run_liquidCNA( timely_patients[x],
                                                                timely = TRUE,
                                                                batch1 = TRUE,
                                                                nbatch = nbatch1[x]))

timely_batch2_results <- sapply(1:7, 
                                simplify = FALSE,
                                function(x) run_liquidCNA( timely_patients[x],
                                                           timely = TRUE,
                                                           batch1 = FALSE,
                                                           nbatch = nbatch2[x]))

names(timely_batch1_results) <- timely_patients
names(timely_batch2_results) <- timely_patients
```

### Problem 2: How can we bring the two batch runs together?

Looking at the results of batch run for patient 840, it is seen that the sample order is no longer in the chronological order. Then, how can we bring the two batch runs together?
```{r load timely runliquidCNA results}
load( "../DATA/timely_results.RData")
timely_batch1_results[[1]]
timely_batch2_results[[1]]
```

### Try dividing into 2 batches of 7 (so that there is higher overlap)

```{r divideTimeSamplesby6, echo = TRUE, eval = FALSE}
#divide time samples into 2 batches of 7 time samples 
#here batch2 is set to 6 as sample number is +1ed in the code
nbatch1b <- 7
nbatch2b <- 6

#initialise list to store results
timely_batch1b_results <- vector(mode = "list", length = length(timely_patients))
timely_batch2b_results <- vector(mode = "list", length = length(timely_patients))

#run liquidCNA on each batch
timely_batch1b_results <- sapply(1:7, 
                                simplify = FALSE, 
                                function(x) run_liquidCNA( timely_patients[x],
                                                                timely = TRUE,
                                                                batch1 = TRUE,
                                                                nbatch = nbatch1b))

timely_batch2b_results <- sapply(1:7, 
                                simplify = FALSE,
                                function(x) run_liquidCNA( timely_patients[x],
                                                           timely = TRUE,
                                                           batch1 = FALSE,
                                                           nbatch = nbatch2b))

#reorder samples so that they are increasing by relative ratio
timely_batch1b_results <- sapply(1:7, function(x)
  timely_batch1b_results[[x]][order(timely_batch1b_results[[x]]$relratio),],
  simplify = FALSE)

timely_batch2b_results <- sapply(1:7, function(x)
  timely_batch2b_results[[x]][order(timely_batch2b_results[[x]]$relratio),],
  simplify = FALSE)

names(timely_batch1b_results) <- timely_patients
names(timely_batch2b_results) <- timely_patients


```

```{r dividedBy6 results}
timely_batch1b_results[[1]]
timely_batch2b_results[[1]]

```

### only odd numbers

```{r oddNumSampesOnly, echo = TRUE, eval = FALSE}
#initialise list to store results
timely_batchc_results <- vector(mode = "list", length = length(timely_patients))

#run liquidCNA on each batch
timely_batchc_results <- sapply(1:7, 
                                simplify = FALSE, 
                                function(x) run_liquidCNA( timely_patients[x],
                                                                timely = TRUE,
                                                                batch1 = TRUE,
                                                                nbatch = NULL))
#reorder samples so that they are increasing by relative ratio
timely_batchc_results <- sapply(1:7, function(x)
  timely_batchc_results[[x]][order(timely_batchc_results[[x]]$relratio),],
  simplify = FALSE)
names(timely_batchc_results) <- timely_patients

```

```{r oddSub results}
timely_batchc_results[[1]]
```

# 3. Relative subclonal ratio

Solon suggested try dividing by total number of sample rather than topSample in estimateRSegmentRatio().

```
for (samp in toEstimate){
      tmp <- c()
      for (top in topSamples){

          tmp <- c(tmp,(seg.ratios[,samp])/(seg.ratios[,top]))
            #simple ratio of purity corrected CNs of segment
            # = (non-top-sample / top-sample)

          }
      final.ratios <- rbind(final.ratios, data.frame(value=tmp, variable=samp, weight=w))

  }
  return(final.ratios)
```

I think in the package CNs of segments are ratioed against topSample as it allows CN to be evaluated segment by segment. I am guessing that this will result less bias for extreme (large/small) dCN of some segments. Which must also be why they are doing medians > means as well.

```{r alternative to estimateRSegmentRatio}
load("../DATA/eg_seg.dcn.toUse.RData")

#input into estimateRSegmentRatio()
#this is dCN of each segment between time sample n and base sample
#order of time sample is permutation w/ greatest number of subclonal segments
seg.dcn.toUse

#Current liquidCNA
topSample <- names(seg.dcn.toUse)[ncol(seg.dcn.toUse)]
toEstimate <- setdiff(names(seg.dcn.toUse), c(topSample,baseSample))
final.ratios <- estimateRSegmentRatio(seg.dcn.toUse, toEstimate, topSample, 1)
final.medians <- aggregate(final.ratios$value, 
                           by=list(final.ratios$variable), median)
final.medians

#mean
mean(seg.dcn.toUse[,1])
mean(seg.dcn.toUse[,2])
mean(seg.dcn.toUse[,1]) / mean(seg.dcn.toUse[,2])

#median
median(seg.dcn.toUse[,1]/nrow(seg.dcn.toUse))
median(seg.dcn.toUse[,2]/nrow(seg.dcn.toUse))
median(seg.dcn.toUse[,1]/nrow(seg.dcn.toUse)) / median(seg.dcn.toUse[,2]/nrow(seg.dcn.toUse))


seg.dcn.toUse
nrow(seg.dcn.toUse)



```

Could explore how accurate this is with synthetic data. Additionally, this method will allow relative ratio to be found for two-sample-only patients (by comparing to )

# 4. Absolute subclonal ratio

### estimateRGaussianFit()

Density of CNs of selected segments are thought as a mixture model of 5 CNs. A Gaussian mixture model is fitted for each sample's segment dCNs. Absolute ratio is defined as the shared mean parameters.

Input:

* seg.rel.toUse = normalised, purity-corrected segment CN of subclonal segments only
* topSample = sample to be investigated - use in loop to evaluate for all samples
* final.medians = output data frame with column "time' denoting the sample and "rat" denoting the estimated ratio and "rat_sd" its variance
* (optional) nStates = number of DeltaCN states presented in the data, e.g. DCN={-1,1,2} -> nStates=3; this is used to compute the variance of the estimate. **default is 3**

```
estimateRGaussianFit <- function(seg.rel.toUse, topSample, 
                                final.medians, gaussSigma=0.1, nStates=2, 
                                lambdaOverwrite=0.2, verbose=F){
    seg.top <- seg.rel.toUse[,topSample]
    start <- mean(seg.top[seg.top<0])
      #average of deltaCNs of top sample that are negative
      
    if (is.nan(start)){
        # estimate the initial value for fitting from segments with DCN between 0 and 1
      start <- -1* mean(seg.top[seg.top>0 & seg.top<1])
  }
  if (is.nan(start)){ 
  # if no starting point can be found, start from a default of 25%
      start <- -0.25
  }
  
  # By default the following DCN values are used: -2, -1, 1, 2, 3
  mixfit <- normalmixEM(as.numeric(seg.top),
                        lambda=lambdaOverwrite,
                        mean.constr = c("2a","a","-a","-2a","-3a"),
                        mu=c(2,1,-1,-2,-3)*start,sigma=gaussSigma)
  # only return the obtained value if converged in less than 800 iterations
  if (length(mixfit$all.loglik)<800){
      final.medians$rat[final.medians$time==topSample] <- -1*(mixfit$mu[2])
      #sigma depends on the number of segments used and that depends on how many states they were distributed across
      final.medians$rat_sd[final.medians$time==topSample] <- (mixfit$sigma[1])/sqrt(nrow(seg.rel.toUse)/nStates)
  }
  if (verbose){
  	print(mixfit$lambda)
  }
  return(final.medians)
}
```

### Density plot of segments' dCNs

```{r understanding absolute}

#input into estimateRSegmentRatio()
plot(density(seg.dcn.toUse[,1]), main = "Histograme of seg.top in estimateRGaussianFit()")

```

### normalmixEM()

Subfunction from mixtools package within estimateRSegmentRatio() to fit distribution to a mixture model:

**normalmixEM(as.numeric(seg.top), lambda=lambdaOverwrite, mean.constr = c("2a","a","-a","-2a","-3a"), mu=c(2,1,-1,-2,-3) x start,sigma=gaussSigma)** 

Input:

* lambda: Initial value of mixing proportions. Automatically repeated as necessary to produce a vector of length k. **Default in the package is 0.2**
* mu: Starting value of vector of component means. **Default is (2,1,-1,-2,-3) x start**. This results *k*, number of components, to be set to 5. Question is why those five dCN states, and why is start the mean of negative dCNs?
* mean.constr: Equality constraints on the mean parameters
*sigma: Starting values of components' standard deviations

**Question regarding EM starting values** k, the number of mixture component, is set to 5 by default. The initial starting values of mean are set as (2,1,-1,-2,-3) x start where start is the mean of negative CNs. 

### normalmixEM() for one time sample with liquidCNA defaults

```{r understanding normalmixEM0, eval = TRUE, echo = FALSE}
gaussSigma=0.1
nStates=3
lambdaOverwrite=0.2
verbose=F

seg.top <- seg.dcn.toUse[,2]
start <- mean(seg.top[seg.top<0])
  #average of deltaCNs of top sample that are negative
  
  if (is.nan(start)){
    # estimate the initial value for fitting from segments with DCN between 0 and 1
    start <- -1* mean(seg.top[seg.top>0 & seg.top<1])
  }
  if (is.nan(start)){ 
    # if no starting point can be found, start from a default of 25%
    start <- -0.25
  }

```

```{r liquidCNA normalmixEM}

mixfit_dcnRemoved <- normalmixEM(as.numeric(seg.top),
                        lambda=lambdaOverwrite,
                        mean.constr = c("2a","a","-a","-2a","-3a"),
                        mu=c(2,1,-1,-2,-3)*start,sigma=gaussSigma)

mixfit_notRemoved <- normalmixEM(as.numeric(seg.top),
                        lambda=lambdaOverwrite,
                        mean.constr = c("2a","a","-a","-2a","-3a"),
                        mu=c(2,1,-1,-2,-3)*start,sigma=gaussSigma)

mixfit_res <- t(data.frame(lambda = mixfit$lambda, mu = mixfit$mu, mixfit$sigma))
mixfit_res
```

```{r}
pdf(file="Fig_17_GMM.pdf", width = 7, height = 8.3)
par(mfrow=c(2,1))

plot(mixfit_notRemoved,which = 2, xlab2 = "Segment dCN values", 
     main2 = "\n \n all dCN included \n ")
abline(v=1.24842543874735, col = "red", lty = 2)
#abline(v=0.9325073, col = "red", lty = 2)
legend("topright", lty = 2, col = "red", legend = "Subclonality = 1.248", bty = "n")

plot(mixfit_dcnRemoved,which = 2, xlab2 = "Segment dCN values", 
     main2 = "\n \n dCN > 5 removed \n \n ")
#abline(v=1.24842543874735, col = "red", lty = 2)
abline(v=0.9325073, col = "red", lty = 2)
legend("topright", lty = 2, col = "red", legend = "Subclonality = 0.933", bty = "n")

title("\n \n Subclonality estimation with GMM | Patient 2734 Sample3", outer = T)

dev.off()

pdf(file="Fig_17_GMM_h.pdf", width = 11, height = 5.5)
par(mfrow=c(1,2))

plot(mixfit_notRemoved,which = 2, xlab2 = "Segment dCN values", 
     main2 = "\n \n all dCN included ")
abline(v=1.24842543874735, col = "red", lty = 2)
#abline(v=0.9325073, col = "red", lty = 2)
legend("topright", lty = 2, col = "red", legend = "Subclonality = 1.248", bty = "n")

plot(mixfit_dcnRemoved,which = 2, xlab2 = "Segment dCN values", 
     main2 = "\n \n dCN > 5 removed")
#abline(v=1.24842543874735, col = "red", lty = 2)
abline(v=0.9325073, col = "red", lty = 2)
legend("topright", lty = 2, col = "red", legend = "Subclonality = 0.933", bty = "n")

title("\n \n Subclonality estimation with GMM | Patient 2734 Sample3", outer = T)

dev.off()
```


### normalmixEM() for one time sample with mixtool defaults

```{r default normalmixEM}

raw_mixfit <- normalmixEM(as.numeric(seg.top), k = 5)
plot(raw_mixfit, which = 2)

raw_mixfit_res <- t(data.frame(lambda = raw_mixfit$lambda, mu = raw_mixfit$mu, raw_mixfit$sigma))
raw_mixfit_res

```

TODO: figure out which patients result NA in absolute derivation and find pattern.

```{r non-convergent patient, echo = FALSE}
# p_num <- 76
# seg.df <- read.delim(paste0("../DATA/2_QDNA_CNout/segment_", patient_ids[p_num], ".txt"))
#   cn.df <- read.delim(paste0("../DATA/2_QDNA_CNout/raw_cn_", patient_ids[p_num], ".txt"))
#   
#   seg.df <- seg.df[,-(1:4)]
#   cn.df <- cn.df[,-(1:4)]
#   
#   colnames(seg.df) <- paste0("Sample", 1:ncol(seg.df))
#   colnames(cn.df) <- paste0("Sample", 1:ncol(cn.df))
#   
#   if(timely == TRUE){
#     if(batch1 == TRUE){
#       seg.df <- seg.df[,1:nbatch]
#       cn.df <- cn.df[,1:nbatch]
#     } else {
#       seg.df <- seg.df[,tail(1:ncol(seg.df), nbatch + 1)]
#       cn.df <- cn.df[,tail(1:ncol(cn.df), nbatch + 1)]
#     }
#   }
#   
#   #optional
#   #re-normalise raw and segmented CN values to be centred at CN=2
#   reNorm <- centreSegs(seg.df)
#   seg.df <- as.data.frame(t(t(seg.df)/reNorm)*2)
#   cn.df <- as.data.frame(t(t(cn.df)/reNorm)*2)
#   
#   #Plot the CN distribution of each sample to gain a quick overview
#   explore_plot <- ggplot(reshape2::melt(seg.df), aes(x=value,y=..scaled.., colour=variable)) +
#     geom_density(adjust=1) +
#     theme_bw() + scale_x_continuous(limits=c(0.5, 5)) +
#     labs(x='Segment copy number',y='Density',colour='') + 
#     ggtitle(paste0('Patient ', patient_ids[p_num]))
#   print(explore_plot)
#   
#   #generate a dataframe of ensemble segments
#   #contiguous sections of bins that are constant in ALL samples
#   segchange <- sapply(1:(nrow(seg.df)-1),
#                       function(i) sum(seg.df[i,]!=seg.df[(i+1),])>0)
#   seg.data <- data.frame(start=c(1,which(segchange)+1),
#                          end=c(which(segchange),length(segchange)+1))
#   seg.data$length <- seg.data$end - seg.data$start+1
#   
#   #Filter out small segments
#   seg.sub <- subset(seg.data, length>12) #12 500kb bins
#   cat("Patient",patient_ids[p_num], '| Total number of segments retained: ',nrow(seg.sub))
#   
#   #Update segment values by fitting a normal distribution to bins in the segment
#   seg.fit <- getNormalFitSegments(seg.sub, cn.df)
#   
#   seg.cns <- seg.fit[[1]]
#   names(seg.cns) <- names(seg.df)
#   
#   seg.df.upd <- data.frame(matrix(NA, ncol=ncol(seg.df), nrow=nrow(seg.df)))
#   names(seg.df.upd) <- names(seg.df); row.names(seg.df.upd) <- row.names(seg.df)
#   
#   for(i in 1:nrow(seg.sub)){
#     seg.df.upd[seg.sub$start[i]:seg.sub$end[i],] <- seg.cns[i,]
#   }
#   
#   ##########################################
#   ########### Purity Estimation ############
#   ##########################################
#   w = c(0.8,1,1,0.15,0.05) #weights of different CN states
#   maxCN=8 #assumed maximum CN 
#   adjVec = c(0.5,0.6,0.8,0.9,1,1.2,1.3,1.5,1.8,2) #smoothing kernel adjustments
#   pVec = seq(0.05, 0.5, by=0.005) #range of purity values to be evaluated
#   
#   #Estimated optimal purity values stored in pHat.df
#   #purity vals that minimise mean and median
#   pHat.df <- data.frame(matrix(vector(),ncol=ncol(seg.df.upd), nrow=2))
#   names(pHat.df) <- names(seg.df.upd)
#   row.names(pHat.df) <- c('mean','median')
#   
#   #The plots show the error of the fit over the range of purity values 
#   for(i in 1:ncol(seg.df.upd)){
#     #for each time sample...
#     
#     x <- na.omit(seg.df.upd[,i]) #get rid of NA rows (i.e., segments with NAs)
#     
#     pFits <- as.data.frame(sapply(adjVec,
#                                   function(a) sapply(pVec,
#                                                      function(p) evalPurityDensity(p,w,a,x,maxCN))))
#     #each row: the 1 purity value evaluated
#     #each column: 1 adjVec, a smoothing kernel adjustment
#     #each value: error for given purity/adjVec
#     
#     pFits$p <- pVec
#     mins <- c(pVec[which.min(apply(pFits[,1:length(adjVec)],1,mean))],
#               pVec[which.min(apply(pFits[,1:length(adjVec)],1,median))])
#     #which row (purity) has minimum mean error and minimum median error?
#     
#     plF <- ggplot(melt(pFits,id='p'), aes(x=p, y=value)) + geom_point(size=2, alpha=0.3)+
#       theme_bw() +
#       geom_vline(xintercept = mins, linetype=c('dashed','dotted'), colour=c('red','blue')) +
#       labs(title= paste0('Patient ', patient_ids[p_num], " | ", names(seg.df.upd)[i]))
#     print(plF)
#     pHat.df[,names(seg.df.upd)[i]] <- mins
#   }
#   
#   #Correct CN values by purity
#   #removing CN contamination by normal
#   #eq being used: CN_corr = 1/p * (CN-2) + 2
#   pVec <- as.numeric(pHat.df[1,,drop=T])
#   seg.df.corr <- as.data.frame(t(t(seg.df.upd-2)*1/pVec)+2)
#   seg.cns.corr <- as.data.frame(t(t(seg.cns-2)*1/pVec)+2)
#   
#   #remove low purity samples if number of number of samples is many
#   # if( length(pVec) > 6 ){
#   #   purity_threshold <- 0.1
#   #   abovePurTh <- pVec >= purity_threshold
#   #   seg.df.corr <- seg.df.corr[,abovePurTh]
#   #   seg.cns.corr <- seg.cns.corr[,abovePurTh]
#   # }
#   
#   #Plot purity-corrected segment distribution
#   purity_plot <- ggplot(melt(as.data.frame(seg.df.corr)), aes(x=value, colour=variable)) +
#     geom_density(adjust=1) + theme_bw() +
#     scale_x_continuous(limits=c(0,8)) + geom_vline(xintercept = 1:6) +
#     labs(x='Purity-corrected segment CN',y='Density',colour='') + 
#     ggtitle(paste0('Patient ', patient_ids[p_num]))
#   print(purity_plot)
#   
#   #Designate reference sample
#   #& calculate dCN
#   baseSample <- names(seg.df.corr)[1]
#   seg.dcn <- seg.cns.corr - seg.cns.corr[,baseSample]
#   seg.dcn.nonBase <- seg.dcn %>% select(-one_of(baseSample))
#   
#   #Choose samples to investigate (all non-base samples by default) 
#   colToUse <- names(seg.dcn.nonBase)
#   #generate all possible permutations
#   nCol <- length(colToUse)
#   seg.dcn.toOrder <- seg.dcn.nonBase[,colToUse]
#   ordVec <- permutations(nCol,nCol,colToUse)
#   
#   # #Permutation is too long
#   # if( nrow(ordVec) > 1000 ){
#   #   
#   #   chrono <- which(apply(ordVec, 1, function(x) identical(x, colToUse)))
#   #   nonchrono <- (1:nrow(ordVec))[-chrono]
#   #   rows <- sample(nonchrono, 1000, replace = F)
#   #   subOrdVec <- ordVec[rows,]
#   #   subOrdVec <- rbind(subOrdVec, ordVec[chrono,])
#   #   ordVec <- subOrdVec
#   #   
#   # }
#   
#   #Initial filtering to find best sample order 
#   #Set parameters to be used when evaluating sample order and segment monotony
#   filterMethod <- 'sd' #filter by saying... if sd < theta, segment = clonal
#   cutOffVec <- seq(0.025,0.5,by=0.005) #thetas to try for initial-filtering
#   epsilon <- 0.05 #error margin for monotony
#   
#   #Diagnostic plot
#   #at different thetas shows:
#   #the proportion of subclonal segments (of non-clonal segments)
#   #total number of non-clonal (light blue)
#   #and subclonal (dark blue) segments
#   fitInfo <- list()
#   for (cutOff in cutOffVec){
#     seg.dcn.Eval <- filterSegmentRatios(seg.dcn.toOrder, cutOff, filterMethod, 0)
#     #Filter relative segment values (compared to baseline) to discard clonal 
#     #(nonchanging) segments that stay relatively constant
#     #Returns: a filtered set of a segments that have values exceeding the threshold
#     
#     best <- findBestOrder(seg.dcn.Eval, ordVec, epsilon, nCol, 0)
#     #Input: segments where clonal segments are initially filtered
#     #Returns: 
#     #$cons: row-names of all segments considered, 
#     #max: maximum fit gained, 
#     #$ord: order with the maximum fit, 
#     #$segs:segments monotone according to max fit
#     fitInfo[[as.character(cutOff)]] <- best
#   }
#   
#   fitInfo.df <- data.frame(cutOff = cutOffVec,
#                            maxFit = sapply(fitInfo, function(x) x$max),
#                            segsAboveCut = sapply(fitInfo, function(x) length(x$cons)),
#                            segsInOrder = sapply(fitInfo, function(x) max(sapply(x$segs, length))))
#   p1 <- ggplot(fitInfo.df, aes(x=cutOff, y=maxFit)) + geom_line(size=2,colour='darkgreen') + theme_bw() +
#     labs(x='Clonal cut-off',y='Subclonal proportion (of non-clonal segments)') + 
#     ggtitle(paste0('Patient ', patient_ids[p_num]))
#   p2 <- ggplot(fitInfo.df, aes(x=cutOff, y=segsAboveCut)) + geom_line(size=2,colour='deepskyblue3') +
#     geom_line(aes(y=segsInOrder), size=2,colour='dodgerblue4') + theme_bw() +
#     labs(x='Clonal cut-off',y='Total number of non-clonal and subclonal segments') + 
#     ggtitle(paste0('Patient ', patient_ids[p_num]))
#   grid.arrange(p1,p2,nrow=1)
#   #p1: proportion of subclonal segments
#   #p2: total number of non-clonal (light blue) and subclonal (dark blue) segments
#   
#   #A typical cut-off ~0.1 is adequate
#   #too low or too high cut-off will select for noise
#   #ideal cut-off should have a reasonable number of subclonal segments (5-20) 
#   #while maximises the subclonal proportion
#   
#   #optimal cut-off for wanted  number of segment
#   minSegmentNumber <- 9
#   recommendCutOff <- getCutOffAuto(fitInfo.df, minSegmentNumber)
#   cat('Recommended cut-off is: ', recommendCutOff)
#   
#   #plot DeltaCN for chosen cut off + ordering + segment classification
#   cutOff <- recommendCutOff
#   fit <- fitInfo[[as.character(cutOff)]]
#   
#   for (ind in 1:nrow(fit$ord)){
#     seg.plot <- seg.dcn[,rev(c(fit$ord[ind,],baseSample))]
#     seg.plot$id <- row.names(seg.plot)
#     seg.plot$filtered <- seg.plot$id %in% fit$cons
#     seg.plot$order <- seg.plot$id %in% fit$segs[[ind]]
#     
#     p <- ggplot(melt(seg.plot, id=c('id','filtered','order')), aes(x=variable, y=value, group=id, colour=paste0(filtered,' | ',order))) +
#       geom_line(size=1.2, alpha=0.75) + theme_bw() +
#       scale_colour_manual(values=c('grey70','#487a8b','firebrick3'), labels=c('clonal','unstable','subclonal')) +
#       labs(x='',y='Change in segment CN (DeltaCN)',colour='Segment type') + 
#       ggtitle(paste0('Patient ', patient_ids[p_num]))
#     print(p)
#   }
#   
#   #if more than one sample order exists
#   ordInd <- 1
#   seg.dcn.toUse <- seg.dcn[fit$segs[[ordInd]],rev(fit$ord[ordInd,])]
  
```

```{r nonconvergent2}

# topSample = "Sample2"
# gaussSigma=0.1
# nStates=3
# lambdaOverwrite=0.2
# verbose=F
# 
# seg.top <- seg.dcn.toUse[,topSample]
# start <- mean(seg.top[seg.top<0])
# start
# 
# # By default the following DCN values are used: -2, -1, 1, 2, 3
# #function for fitting of mixture distributions
# mixfit <- normalmixEM(as.numeric(seg.top),
#                       lambda=lambdaOverwrite,
#                       mean.constr = c("2a","a","-a","-2a","-3a"),
#                       mu=c(2,1,-1,-2,-3)*start,
#                       sigma=gaussSigma)
# plot(mixfit,which = 2)
# 
# mixfit2 <- normalmixEM(as.numeric(seg.top), k = 5)
# plot(mixfit2, which = 2)


```



